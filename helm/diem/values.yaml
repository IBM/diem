ingress:
  ## name is the name of your ingress file
  name: diem-ingress

  ## the name of the version v1 or v1beta
  version: v1

  ## tls will be the name of your tls secret. It will be created for you if don't have one
  ## Otherwise get the tls from your cluster
  ## kubectl describe ingress diem-ingress
  ##  TLS: diem-tls-secret terminates example.com
  createtls: true
  tls: diem-tls-secret

  ## host is the hostname of your application
  host: example.com
  ## proxy is the name of your proxy host
  ## this can happen when you run in a cluster and have a proxy url
  proxy: ""

common:
  ## name is the name of the deployment
  name: diem-common

  config:
    diemAdminRbac: diem-rbac
    diemAdmin: diem-admin
    K8_SYSTEM_NAME: "Diem on CRC"
    K8_SYSTEM: uat
    K8_INSTANCE: US

    ## the port all pods will be running under
    PORT: 8192

  secrets:
    ## secretmapref is the name of the common secret map ref
    SLACKHOOK: ""

## these are the settings for Diem Core
core:
  ## name is the name of the deployment
  name: diem-core

  ## this relates to the diem-core deployment
  deployment:
    ## socketName is the url the client ui socket will connect to
    ## for the moment Do Not Change this
    socketName: etl-socket-server
    tier: backend
    version: 1.0.0
    image: quay.io/diem/core
    replicas: 1
    port: 80

  config:
    ## important, the path the application will run under
    APPPATH: /etl-mgr

    ## if core runs with a dedicated service account
    serviceAccountName: ""

    spark:
      SPARK_IMAGE: quay.io/diem/pyspark:3.1.1-rc1
      SPARK_IMAGEPULLSECRETS: ""
      OPERATOR_DRIVER_CORES: "2"
      OPERATOR_DRIVER_MEMORY: 1024m
      OPERATOR_EXECUTOR_CORES: 8
      OPERATOR_EXECUTOR_INSTANCES: 2
      OPERATOR_EXECUTOR_MEMORY: 8G

    volume:
      volumeName: spark-data
      volumeMountPath: /shared
      volumeClaimName: spark-shared-data
      storageClassName: ""

    ## Slack integration, enable it by putting the enabled to true
    slack:
      enabled: false
      SLACK_EMOJI: ":diem:"
      SLACK_DEPLOY_CHANNEL: "#diem-deploy-uat"
      SLACK_DEPLOY_USERNAME: "diem- (Notification)"
      SLACK_INTERNAL_CHANNEL: "#diem-bugs-uat"
      SLACK_INTERNAL_USERNAME: "diem - (Internal)"
      SLACK_USER_CHANNEL: "#diem-bugs-uat"
      SLACK_USER_USERNAME: "diem - (User)"

    ## Integrate your own cloud object storage
    ## leave this blank if you want to use minio integrated as a dependency
    ## if these value are going to be used, disable minio
    s3:
      enabled: false
      apiKeyId: ""
      endpoint: ""
      serviceInstanceId: ""
      signatureVersion: ""

  ##
  secrets:
    SESSION_NAME: etl.sid
    SESSION_SECRET: ETLSECRETPW
    MONGO_URL: ""
    MONGO_CA: ""
    SENDGRID_API: ""
    JWTTOKEN: ""

  ## OpenId authentication
  auth:
    clientId: ""
    clientSecret: ""
    discoveryUrl: ""
    callbackUrl: /sso/callback

nodepy:
  ## name of the deployment
  name: diem-nodepy

  ## image is the the name of the nodepy image
  image: quay.io/diem/nodepy
  port: 80
  replicas: 1
  tier: backend
  version: 1.0.0

## cnfiguration for the operator
operator:
  ## name of the deployment
  name: diem-operator

  ## any service accountname
  serviceAccountName: ""

  ## image is the the name of the nodepy image
  image: quay.io/diem/operator
  port: 80
  replicas: 1
  tier: backend
  version: 1.0.0

## enable the spark Operator
## for more details see
## https://github.com/GoogleCloudPlatform/spark-on-k8s-operator
spark-operator:
  ## overwrite the spark job namespace if needed, otherwise it will
  ## run in the default namespace
  sparkJobNamespace: ""

  ## enables the spark webhoob
  enableWebhook: true
  serviceAccounts:
    spark:
      name: spark
  image:
    repository: quay.io/diem/spark-operator
    tag: 2.0.0

mongodb:
  createservice: true
  auth:
    username: diemadmin
    rootPassword: diemadmin
    password: diempassword
    database: ibmclouddb

## Nats
## see https://github.com/bitnami/charts/tree/master/bitnami/nats

nats:
  createservice: true
  ## the name of the service
  name: nats

  nameOverride: diem-nats

  ## authentication
  ## it4s recommended you create your own long encrypted password
  ## follow the instructions here https://docs.nats.io/nats-tools/natscli
  ## either fill in the password in plain text or
  ## encrypt your password and fill in your plain password in the unencrypted_password
  ## if you do not want to use an encrypted password leave the unencrypted_password blank
  ## example
  ##    unencrypted_password: natsuserauthorisationpassword
  ##    password: $2a$11$4X.WN5r4PeaB4vYMtbFsEOKJTSxEiWuCq/FV25RsWZvLFSEeDOrf6
  ##
  auth:
    enabled: false
    user: ""
    password: ""

  ## enable external access, this will create a load balancer
  ## you can get your ip from >_ kubectl get svc diem-nats-lb -o wide
  external: false

  ## only if external is enabled (will be your loadbalancer ip)
  ## you might need to reapply the helm chart twice once you have it
  ip: diem-nats

  priorityClassName: system-node-critical

  replicaCount: 1

  nats:
    image: nats:2.2.1-alpine3.13
    pullPolicy: IfNotPresent
  cluster:
    ## the name of the cluster
    name: diem-cluster
    ## enable clustering
    enabled: true
    ## number of replicas to start
    replicas: 3
    noAdvertise: true
  natsbox:
    enabled: false
  reloader:
    enabled: false
  # Prometheus NATS Exporter configuration.
  exporter:
    enabled: false

minio:
  createservice: true
  accessKey: diemadmin
  secretKey: diempassword

  persistence:
    ## use an existing claim
    # existingClaim: spark-shared-data
    # VolumeName: spark-data

    ## depending on your capacity you can increase this amount
    size: 20Gi

  mountPath: /minio
  securityContext:
    enabled: false
  resources:
    requests:
      memory: 1Gi

## encode all secrets base64
base64: true

## pod annotations
controller:
  podAnnotations:

## Other specific implementations
## This will tell the plugin where to find vault values
avp_path: ""
